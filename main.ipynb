{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb56fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import hstack\n",
    "import warnings\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "max_int = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        csv.field_size_limit(max_int)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        max_int = int(max_int / 10)\n",
    "\n",
    "\n",
    "def create_detailed_features(df):\n",
    "    df = df.copy()\n",
    "    for col in ['author', 'title', 'abstract']:\n",
    "        df[col] = df[col].fillna('')\n",
    "    \n",
    "    df['journal'] = df['Id'].str.extract(r'^\\d{4}(\\w{1,5})\\.+', expand=False).fillna('UNKNOWN')\n",
    "    df['author_count'] = df['author'].str.count(',') + 1\n",
    "    df['abstract_length'] = df['abstract'].str.len()\n",
    "\n",
    "    df['chandra_in_title'] = df['title'].str.contains('chandra', case=False, na=False).astype(int)\n",
    "    df['jwst_in_title'] = df['title'].str.contains('jwst|james webb', case=False, na=False).astype(int)\n",
    "    df['hst_in_title'] = df['title'].str.contains('hst|hubble', case=False, na=False).astype(int)\n",
    "    df['chandra_in_abstract'] = df['abstract'].str.contains('chandra', case=False, na=False).astype(int)\n",
    "    df['jwst_in_abstract'] = df['abstract'].str.contains('jwst|james webb', case=False, na=False).astype(int)\n",
    "    df['hst_in_abstract'] = df['abstract'].str.contains('hst|hubble', case=False, na=False).astype(int)\n",
    "    \n",
    "    median_year = df['year'].median()\n",
    "    df['year'] = df['year'].fillna(median_year)\n",
    "    return df\n",
    "\n",
    "print(\"loading data...\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('train.csv', dtype={'Id': str}, engine='python')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: csv not found.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "print(\"\\nCreating detailed features...\")\n",
    "df = create_detailed_features(df)\n",
    "\n",
    "\n",
    "print(\"\\nPreprocessing data...\")\n",
    "le_journal = LabelEncoder()\n",
    "df['journal'] = le_journal.fit_transform(df['journal'])\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=150, stop_words='english', ngram_range=(1,2))\n",
    "tfidf_features = tfidf.fit_transform(df.title + ' ' + df.abstract)\n",
    "\n",
    "numeric_features = [\n",
    "    'year', 'journal', 'author_count', 'abstract_length',\n",
    "    'chandra_in_title', 'jwst_in_title', 'hst_in_title',\n",
    "    'chandra_in_abstract', 'jwst_in_abstract', 'hst_in_abstract'\n",
    "]\n",
    "X = hstack([df[numeric_features].values, tfidf_features])\n",
    "y = df['telescope']\n",
    "print(\"Data preparation complete.\")\n",
    "\n",
    "print(\"\\nOptimizing model with RandomizedSearchCV...\")\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300, 400],\n",
    "    'max_depth': [15, 20, 25, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,  # Try 20 different combinations\n",
    "    cv=skf,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "random_search.fit(X, y)\n",
    "\n",
    "print(f\"\\nBest parameters found: {random_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {random_search.best_score_:.4f}\")\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "print(\"\\nCreating final submission file...\")\n",
    "\n",
    "predictions = best_model.predict(X)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': df['Id'],\n",
    "    'telescope': predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('final_submission_task1.csv', index=False)\n",
    "\n",
    "print(\"\\nsubmission file is created.\")\n",
    "print(\"First 5 rows of your submission:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bebed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nowwe are saving the model and preprocessed data\n",
    "import joblib\n",
    "\n",
    "print(\"\\nSaving the final model and all necessary preprocessors...\")\n",
    "\n",
    "model_components = {\n",
    "    'model': best_model,\n",
    "    'label_encoder_journal': le_journal,\n",
    "    'tfidf_vectorizer': tfidf,\n",
    "    'numeric_features_list': numeric_features\n",
    "}\n",
    "\n",
    "joblib.dump(model_components, 'final_model_components.joblib')\n",
    "\n",
    "print(\"Model and components saved successfully to 'final_model_components.joblib'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06f4e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, finally we test our model with the given test.csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "from scipy.sparse import hstack\n",
    "import warnings\n",
    "import csv \n",
    "import sys  \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "max_int = sys.maxsize\n",
    "while True:\n",
    "    try:\n",
    "        csv.field_size_limit(max_int)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        max_int = int(max_int/10)\n",
    "\n",
    "def create_detailed_features(df):\n",
    "    \"\"\"Creates a rich set of features from the available data.\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in ['author', 'title', 'abstract']:\n",
    "        df[col] = df[col].fillna('')\n",
    "    df['journal'] = df['Id'].str.extract(r'^\\d{4}(\\w{1,5})\\.+', expand=False).fillna('UNKNOWN')\n",
    "    df['author_count'] = df['author'].str.count(',') + 1\n",
    "    df['abstract_length'] = df['abstract'].str.len()\n",
    "    df['chandra_in_title'] = df['title'].str.contains('chandra', case=False, na=False).astype(int)\n",
    "    df['jwst_in_title'] = df['title'].str.contains('jwst|james webb', case=False, na=False).astype(int)\n",
    "    df['hst_in_title'] = df['title'].str.contains('hst|hubble', case=False, na=False).astype(int)\n",
    "    df['chandra_in_abstract'] = df['abstract'].str.contains('chandra', case=False, na=False).astype(int)\n",
    "    df['jwst_in_abstract'] = df['abstract'].str.contains('jwst|james webb', case=False, na=False).astype(int)\n",
    "    df['hst_in_abstract'] = df['abstract'].str.contains('hst|hubble', case=False, na=False).astype(int)\n",
    "    median_year = df['year'].median()\n",
    "    df['year'] = df['year'].fillna(median_year)\n",
    "    return df\n",
    "\n",
    "print(\"Loading model and components...\")\n",
    "components = joblib.load('final_model_components.joblib')\n",
    "model = components['model']\n",
    "le_journal = components['label_encoder_journal']\n",
    "tfidf = components['tfidf_vectorizer']\n",
    "numeric_features = components['numeric_features_list']\n",
    "print(\"Components loaded successfully.\")\n",
    "\n",
    "\n",
    "print(\"\\nLoading and preparing test.csv...\")\n",
    "\n",
    "test_df = pd.read_csv('test.csv', dtype={'Id': str}, engine='python')\n",
    "test_df = create_detailed_features(test_df)\n",
    "print(\"Features created for test data.\")\n",
    "\n",
    "print(\"\\nPreprocessing test data...\")\n",
    "\n",
    "test_df['journal'] = test_df['journal'].apply(lambda x: le_journal.transform([x])[0] if x in le_journal.classes_ else -1)\n",
    "tfidf_features_test = tfidf.transform(test_df.title + ' ' + test_df.abstract)\n",
    "X_test = hstack([test_df[numeric_features].values, tfidf_features_test])\n",
    "print(\"test data processed and ready for prediction.\")\n",
    "\n",
    "print(\"\\nMaking predictions using hybrid model...\")\n",
    "\n",
    "main_model_predictions = model.predict(X_test)\n",
    "\n",
    "def get_golden_feature_prediction(id_string):\n",
    "    if '_' in id_string:\n",
    "        suffix = id_string.split('_')[-1].upper()\n",
    "        if suffix in ['CHANDRA', 'HST', 'JWST', 'NONE']:\n",
    "            return suffix\n",
    "    return None\n",
    "\n",
    "golden_feature_predictions = test_df['Id'].apply(get_golden_feature_prediction)\n",
    "\n",
    "final_predictions = []\n",
    "for i in range(len(test_df)):\n",
    "    golden_pred = golden_feature_predictions.iloc[i]\n",
    "    main_pred = main_model_predictions[i]\n",
    "    \n",
    "    if golden_pred is not None:\n",
    "        final_predictions.append(golden_pred)\n",
    "    else:\n",
    "        final_predictions.append(main_pred)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_df['Id'],\n",
    "    'telescope': final_predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission_hybrid.csv', index=False)\n",
    "\n",
    "print(\"\\nHybrid submission file 'submission_hybrid.csv' is created\")\n",
    "\n",
    "print(\"First 10 rows of the final predictions:\")\n",
    "print(submission_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
